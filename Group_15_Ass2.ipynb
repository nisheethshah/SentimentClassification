{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic essential libraries\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "#stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#Sci-kit learn and N\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report,f1_score\n",
    "\n",
    "#Keras (Tensorflow for CNN model generation and Evaluation)\n",
    "from keras.preprocessing import sequence,text\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Embedding,LSTM,Conv1D,GlobalMaxPooling1D,Flatten,MaxPooling1D,GRU,SpatialDropout1D,Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#pd.set_option('display.max_colwidth',100)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfile = pd.read_csv(\"train_data.csv\")\n",
    "testfile = pd.read_csv(\"test_data.csv\")\n",
    "labelfile = pd.read_csv(\"train_label.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renaming temporary Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelfile.head()\n",
    "train_df = pd.DataFrame(columns=['ID','text'])\n",
    "testing_df = pd.DataFrame(columns=['ID','text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-checking the data after transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "train_df['ID'] = trainfile['trn_id']\n",
    "train_df['text'] = trainfile['text']\n",
    "\n",
    "testing_df['ID'] = testfile['test_id']\n",
    "testing_df['text'] = testfile['text']\n",
    "\n",
    "print(len(train_df))\n",
    "print(len(testing_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transferring the Labels for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "650000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainlabel = labelfile['label']\n",
    "len(trainlabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining the Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [train_df, testing_df]\n",
    "combined = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Length of Combined Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "700000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#stopwords\n",
    "stop = stopwords.words('english')\n",
    "#lowercase\n",
    "combined['text']=combined['text'].str.lower()\n",
    "#initialising the tokeniser object\n",
    "tokenise = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['text']=combined['text'].apply(lambda x: tokenise.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700000, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['text']=combined['text'].apply(lambda x: [item for item in x if item not in stop])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-combining the words post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['text'] = combined['text'].apply(' '.join)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "food       408071\n",
       "good       379196\n",
       "place      351781\n",
       "like       293693\n",
       "service    276443\n",
       "get        266153\n",
       "time       264204\n",
       "one        263065\n",
       "would      261263\n",
       "great      246707\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = pd.Series(' '.join(combined['text']).split()).value_counts()[:10]\n",
    "freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trn_1</td>\n",
       "      <td>well got write first review yelp reviews figur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trn_2</td>\n",
       "      <td>greek restaurant tasty tried chicken kabob ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trn_3</td>\n",
       "      <td>website says open google says open yelp says o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trn_4</td>\n",
       "      <td>could give zero stars walked big lady asked ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trn_5</td>\n",
       "      <td>definitely excellent tried mochi mango flavore...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                               text\n",
       "0  trn_1  well got write first review yelp reviews figur...\n",
       "1  trn_2  greek restaurant tasty tried chicken kabob ten...\n",
       "2  trn_3  website says open google says open yelp says o...\n",
       "3  trn_4  could give zero stars walked big lady asked ne...\n",
       "4  trn_5  definitely excellent tried mochi mango flavore..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = list(freq.index)\n",
    "combined['text'] = combined['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing Least Frequent Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "experiensia    1\n",
       "poopeyes       1\n",
       "allrighty      1\n",
       "arrrgggh       1\n",
       "mogador        1\n",
       "normak         1\n",
       "thailands      1\n",
       "nailbar        1\n",
       "teeeeeeny      1\n",
       "sanguinely     1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = pd.Series(' '.join(combined['text']).split()).value_counts()[-10:]\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = list(freq.index)\n",
    "combined['text'] = combined['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the combined DF to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = combined.iloc[:650000,:]\n",
    "df_test = combined.iloc[650000:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transferring values into Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(650000,) (650000,) (650000, 6) (50000,)\n"
     ]
    }
   ],
   "source": [
    "train_text=df_train.text.values\n",
    "test_text=df_test.text.values\n",
    "target = labelfile.label\n",
    "y = to_categorical(target)\n",
    "print(train_text.shape,target.shape,y.shape,test_text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting training data into test and train for model evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(520000,) (520000, 6)\n",
      "(130000,) (130000, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train_text,X_val_text,y_train,y_val=train_test_split(train_text,y,test_size=0.2,stratify=y,random_state=123)\n",
    "print(X_train_text.shape,y_train.shape)\n",
    "print(X_val_text.shape,y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of Unique Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175132"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words=' '.join(X_train_text)\n",
    "all_words=word_tokenize(all_words)\n",
    "dist=FreqDist(all_words)\n",
    "num_unique_word=len(dist)\n",
    "num_unique_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum review length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2005"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_len=[]\n",
    "for text in X_train_text:\n",
    "    word=word_tokenize(text)\n",
    "    l=len(word)\n",
    "    r_len.append(l)\n",
    "    \n",
    "MAX_REVIEW_LEN=np.max(r_len)\n",
    "MAX_REVIEW_LEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the parameters for CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = num_unique_word\n",
    "max_words = MAX_REVIEW_LEN\n",
    "batch_size = 128\n",
    "epochs = 3\n",
    "num_classes=6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing text that can fit and be reused in multiple text documents. After fit, it is encoded for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train_text))\n",
    "X_train = tokenizer.texts_to_sequences(X_train_text)\n",
    "X_val = tokenizer.texts_to_sequences(X_val_text)\n",
    "X_test = tokenizer.texts_to_sequences(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets convert the sequences into 2D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(520000, 2005) (130000, 2005) (50000, 2005)\n"
     ]
    }
   ],
   "source": [
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_val = sequence.pad_sequences(X_val, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "print(X_train.shape,X_val.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a CNN neural model with 1-dimension window. We have one hidden layer and compile the model using ADAM optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 2005, 100)         17513200  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2005, 100)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 2005, 64)          19264     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 17,541,558\n",
      "Trainable params: 17,541,558\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2= Sequential()\n",
    "model2.add(Embedding(max_features,100,input_length=max_words))\n",
    "model2.add(Dropout(0.2))\n",
    "\n",
    "model2.add(Conv1D(64,kernel_size=3,padding='same',activation='relu',strides=1))\n",
    "model2.add(GlobalMaxPooling1D())\n",
    "\n",
    "model2.add(Dense(128,activation='relu'))\n",
    "model2.add(Dropout(0.2))\n",
    "\n",
    "model2.add(Dense(num_classes,activation='softmax'))\n",
    "\n",
    "\n",
    "model2.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the model to find the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 520000 samples, validate on 130000 samples\n",
      "Epoch 1/3\n",
      "520000/520000 [==============================] - 4736s 9ms/step - loss: 0.9665 - acc: 0.5821 - val_loss: 0.8872 - val_acc: 0.6174\n",
      "Epoch 2/3\n",
      "520000/520000 [==============================] - 4755s 9ms/step - loss: 0.8353 - acc: 0.6436 - val_loss: 0.8778 - val_acc: 0.6208\n",
      "Epoch 3/3\n",
      "520000/520000 [==============================] - 4734s 9ms/step - loss: 0.7646 - acc: 0.6772 - val_loss: 0.8895 - val_acc: 0.6188\n",
      "Wall time: 3h 57min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history2=model2.fit(X_train, y_train, validation_data=(X_val, y_val),epochs=epochs, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the labels for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 105s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "ypred = model2.predict_classes(X_test,verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get column names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'text'], dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make final output dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.DataFrame({\"ID\":df_test[\"ID\"],\"label\":ypred})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv(\"output3.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
